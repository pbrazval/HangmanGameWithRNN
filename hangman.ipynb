{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic strategy overview:\n",
    "\n",
    "* For each riddle, given its length n, letters will be guessed according to two possible schemes:\n",
    "\n",
    "  * If no letter has been correctly guessed yet, letters will be guessed based on the frequency of letters in all n-long words in the dictionary.\n",
    "  \n",
    "  * If there have already been some correct guesses, an RNN model will be used to predict the next letter to be guessed.\n",
    "\n",
    "## Table of contents.\n",
    "\n",
    "### 1. Loading the existing dataset as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_list.txt', 'r') as file:\n",
    "    train_data = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating a dictionary of most common letters in the words of each word length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Dropout, LSTM, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import chain, combinations\n",
    "from datetime import datetime\n",
    "\n",
    "def find_most_common_letters(words):\n",
    "    letters_by_length = defaultdict(Counter)\n",
    "\n",
    "    for word in words:\n",
    "        letters_by_length[len(word)].update(word.lower()) \n",
    "        \n",
    "    most_common_letters = {length: [letter for letter, count in letters.most_common(6)]\n",
    "                           for length, letters in letters_by_length.items()}\n",
    "    return most_common_letters\n",
    "\n",
    "most_common_letters = find_most_common_letters(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create dataset for training\n",
    "\n",
    "In order to train the RNN model that follows, synthetic riddles and their respective correct answers will be needed. These will be created according to the steps outlined below:\n",
    "\n",
    "### 3.1. Creating synthetic riddles\n",
    "\n",
    "For each word w in the training vocabulary, composed of a set k of unique letters, we can generate potential riddles. This is done by masking a subset of letters in k, where this subset is an element of the power set of k. Consequently, each element k_i from the power set of k represents a possible riddle, when all the letters in k_i are concealed in the riddle. For each riddle generated, a corresponding guess containing all the omitted letters is created, representing all the possible correct guesses. For example, in one iteration, a riddle \"Co__ect_c_t\" and the correct guess \"niu\" would be created.\n",
    "\n",
    "The function `hangman_guesses(word,n)` below creates riddles from word word, limited to n riddles to avoid an excessive bias towardss words composed of a large number of different letters.  The cardinality of the power set being 2^{len(k)} means that without this limitation the RNN's exposure to shorter words would be restricted. In the example below, n=64.\n",
    "\n",
    "Finally, a subset of 15% of all riddles and guesses were used for the following steps, to ensure feasible training times. This network was trained in an Apple M1 Pro 2021, 8-core, with 16GB RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "recreate_dataset = False\n",
    "recreate_shuffled_dataset = False\n",
    "recreate_shuffled_dataset_from_full = False\n",
    "\n",
    "def substitute_letters(word, letters_to_omit):\n",
    "    word_list = list(word)\n",
    "    \n",
    "    word_list = [\"_\" if char in letters_to_omit else char for char in word_list ]\n",
    "    \n",
    "    return ''.join(word_list)\n",
    "\n",
    "#%%\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    powerset = list(chain.from_iterable(combinations(s, r) for r in range(len(s)+1)))\n",
    "    powerset = [set(k) for k in powerset if (len(k) > 0 and len(k) < len(s))]\n",
    "    random.shuffle(powerset)\n",
    "    return powerset\n",
    "\n",
    "#%%\n",
    "def hangman_guesses(word, n):\n",
    "    wordset = list(set(word))\n",
    "    guesses = []\n",
    "    changed_words = []\n",
    "    subsets = powerset(wordset)\n",
    "    for subset in subsets:\n",
    "        changed_word = substitute_letters(word, subset)\n",
    "        guesses.append(''.join(subset))\n",
    "        changed_words.append(changed_word)\n",
    "    return guesses[0:n], changed_words[0:n]\n",
    "\n",
    "if recreate_dataset:\n",
    "    all_guesses = []\n",
    "    all_changed_words = []\n",
    "    with open('all_guesses_limit64.txt', 'a') as guesses_file, open('all_changed_words_limit64.txt', 'a') as changed_words_file:\n",
    "        for i, word in enumerate(train_data):\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"Processing word {i}\")\n",
    "            guesses, changed_words = hangman_guesses(word, 64)\n",
    "            \n",
    "            # Write the guesses and changed words to their respective files\n",
    "            for guess in guesses:\n",
    "                guesses_file.write(guess + '\\n')\n",
    "            \n",
    "            for changed_word in changed_words:\n",
    "                changed_words_file.write(changed_word + '\\n')\n",
    "\n",
    "if recreate_shuffled_dataset_from_full:\n",
    "    with open('all_changed_words.txt', 'r') as file:\n",
    "        riddles_full = file.read().splitlines() \n",
    "        \n",
    "    with open('all_guesses.txt', 'r') as file:\n",
    "        guesses_full = file.read().splitlines() \n",
    "    lg = len(guesses_full)\n",
    "    idxs = random.sample(range(0, lg), int(lg*0.15))\n",
    "    print(\"Number of samples: \", len(idxs))\n",
    "    riddles = [riddles_full[i] for i in idxs]\n",
    "    print(\"Number of samples: \", len(riddles))\n",
    "    guesses = [guesses_full[i] for i in idxs]\n",
    "    print(\"Number of samples: \", len(guesses))\n",
    "    print(\"Writing riddles and guesses to file\")\n",
    "    with open('riddles15.txt', 'w') as file:\n",
    "        for riddle in riddles:\n",
    "            file.write(riddle + '\\n')\n",
    "    print(\"Writing guesses to file\")\n",
    "    with open('guesses15.txt', 'w') as file:\n",
    "        for guess in guesses:\n",
    "            file.write(guess + '\\n')\n",
    "\n",
    "if recreate_shuffled_dataset:\n",
    "    with open('all_changed_words_limit64.txt', 'r') as file:\n",
    "        riddles_full = file.read().splitlines() \n",
    "        \n",
    "    with open('all_guesses_limit64.txt', 'r') as file:\n",
    "        guesses_full = file.read().splitlines() \n",
    "    lg = len(guesses_full)\n",
    "    idxs = random.sample(range(0, lg), lg)\n",
    "    print(\"Number of samples: \", len(idxs))\n",
    "    riddles = [riddles_full[i] for i in idxs]\n",
    "    print(\"Number of samples: \", len(riddles))\n",
    "    guesses = [guesses_full[i] for i in idxs]\n",
    "    print(\"Number of samples: \", len(guesses))\n",
    "    print(\"Writing riddles and guesses to file\")\n",
    "    with open('riddles_limit64.txt', 'w') as file:\n",
    "        for riddle in riddles:\n",
    "            file.write(riddle + '\\n')\n",
    "    print(\"Writing guesses to file\")\n",
    "    with open('guesses_limit64.txt', 'w') as file:\n",
    "        for guess in guesses:\n",
    "            file.write(guess + '\\n')\n",
    "else:    \n",
    "    with open('riddles_limit64.txt', 'r') as file:\n",
    "        riddles = file.read().splitlines()\n",
    "\n",
    "    with open('guesses_limit64.txt', 'r') as file:\n",
    "        guesses = file.read().splitlines()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Preprocessing the data for the RNN\n",
    "\n",
    "#### 3.2.1. Preprocessing riddles\n",
    "\n",
    "Each riddle underwent TextVectorization, converting each of the possible characters in the riddle to an integer. Each letter is an object of interest, so riddles were split by character. Riddle length was limited to 15 characters to accommodate the majority of the words in the training set.\n",
    "\n",
    "#### 3.2.2. Preprocessing guesses\n",
    "\n",
    "This stage maps each guess, e.g. \"eiu\" in \"Conn_ct_c_t\" to a 1 x 26 vector of probabilities (here, [0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0]), corresponding to the 26 letters of the alphabet. This encoding employs `tf.keras.layers.StringLookup` for multi-hot encoding.\n",
    "\n",
    "The following steps involved:\n",
    "\n",
    "- Removing the first element from each encoded guess, which corresponds to the \"Unknown\" character. This character is present in all encoded guesses and does not contribute information to the RNN.\n",
    "\n",
    "- Converting both guesses and riddles into tensor slices.\n",
    "\n",
    "Each element of the final dataset is a 2-tuple with an encoded riddle and an encoded guess. Samples are processed in 64-unit batches. The prefetching of one batch is added to improve performance.\n",
    "\n",
    "Finally, the dataset is split into a train-set and a test-set, containing respectively 80% and 20% of the data. \n",
    "\n",
    "This setup was demonstrated using the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  28\n",
      "Encoding riddles and guesses\n",
      "Encoding riddles\n",
      "Encoding guesses\n",
      "Encoding complete\n"
     ]
    }
   ],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(output_sequence_length = 15, # Max length of words\n",
    "                                                   output_mode = \"int\",\n",
    "                                                   split = \"character\")\n",
    "text_vec_layer.adapt(riddles[1:10000])\n",
    "print(\"Vocabulary size: \", text_vec_layer.vocabulary_size())\n",
    "\n",
    "# Convert words to lists of characters\n",
    "char_lists = [list(word) for word in guesses]\n",
    "\n",
    "# Pad the sequences to have the same length\n",
    "padded_char_lists = tf.keras.preprocessing.sequence.pad_sequences(char_lists,\n",
    "                                                                  dtype=object, \n",
    "                                                                  padding='post',\n",
    "                                                                  value='')\n",
    "\n",
    "lookup_layer = tf.keras.layers.StringLookup(vocabulary=list(\"abcdefghijklmnopqrstuvwxyz\"),\n",
    "                                            output_mode='multi_hot',\n",
    "                                            pad_to_max_tokens=True,\n",
    "                                            max_tokens=27, \n",
    "                                            sparse=True)\n",
    "print(\"Encoding riddles\")\n",
    "encoded_riddles = text_vec_layer(riddles)\n",
    "print(\"Encoding guesses\")\n",
    "\n",
    "encoded_guesses = lookup_layer(padded_char_lists)\n",
    "print(\"Encoding complete\")\n",
    "encoded_guesses = tf.sparse.to_dense(encoded_guesses)\n",
    "encoded_guesses = encoded_guesses[:,1:]\n",
    "tensor_riddles = tf.data.Dataset.from_tensor_slices(encoded_riddles)\n",
    "tensor_guesses = tf.data.Dataset.from_tensor_slices(encoded_guesses)\n",
    "dsmap = tf.data.Dataset.zip(tensor_riddles, tensor_guesses)\n",
    "dsmap = dsmap.batch(64)\n",
    "dsmap = dsmap.prefetch(1)\n",
    "\n",
    "#%%\n",
    "def is_test(x, _):\n",
    "    return x % 5 == 0\n",
    "\n",
    "def is_train(x, y):\n",
    "    return not is_test(x, y)\n",
    "\n",
    "recover = lambda x, y: y\n",
    "\n",
    "# Split the dataset for training.\n",
    "test_dataset = dsmap.enumerate().filter(is_test).map(recover)\n",
    "\n",
    "# Split the dataset for testing/validation.\n",
    "train_dataset = dsmap.enumerate().filter(is_train).map(recover)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RNN model\n",
    "\n",
    "An RNN model is constructed to estimate the best guess for each riddle.\n",
    "\n",
    "- The number of tokens is set to 28, which is equal to the 26 letters in the English language and two additional tokens (underscore sign and \"unknown\")\n",
    "\n",
    "- Initially, the RNN model is composed of the 256-cell long bidirectional recurrent layers. These bidirectional layers capture dependencies in both directions of the input sequence, which is important to understand the context around missing letters in hangman. \n",
    "\n",
    "- The recurrent dropout of 0.1 in both GRU layers and the subsequent dropout of 0.1 in the following layer are used to prevent overfitting, thus allowing for better performance out of sample (which is especially important since hangman will be played with unknown words)\n",
    "\n",
    "- The dense layer of 128 units and ELU activation provides nonlinearity to the model and helps to learn complex patterns from the data. ELU handles the vanishing gradient problem faster than ReLU and shows faster convergence in the data as well.\n",
    "\n",
    "- The final Dense layer has 26 units, corresponding to the 26 letters of the alphabet, with softmax activation to output a probability distribution over all possible letters.  \n",
    "\n",
    "- The model is initially set to run through a maximum of 100 epochs. EarlyStopping is employed to prevent overfitting by halting training if the model's loss doesn't improve after three epochs, starting from epoch 2, restoring the weights from its best-performing iteration.\n",
    "\n",
    "- The model is optimized using the Nadam optimizer and binary crossentropy loss; which will be minimized as the vector of predicted probabilities approaches the vector of true probabilities mentioned in Section 3.2.2. The precision metric is configured to evaluate the model's performance, specifically measuring the precision of the prediction with highest probability, which is critical in a game like hangman where the first guess significantly influences game progress.\n",
    "\n",
    "- Several hyperparameters were calibrated (number of cells in each layer; number of layers; dropout rates). Training time were also considered.\n",
    "\n",
    "- The model was fit with the trainset, whereas performance is evaluated in unseen riddles (`test_dataset`), since generalizing well to unseen data is fundamental here. The number of `steps_per_epoch` was limited to 25,000 for increased computation speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = 28\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(n_tokens, output_dim=28))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(256, recurrent_dropout = 0.1, return_sequences=True))) \n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(256, recurrent_dropout = 0.1, return_sequences=False)))\n",
    "model.add(tf.keras.layers.Dropout(0.1))  \n",
    "model.add(tf.keras.layers.Dense(128, activation='elu')) \n",
    "model.add(tf.keras.layers.Dense(26, activation='softmax'))\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3, start_from_epoch=2, restore_best_weights=True)\n",
    "model.compile(optimizer=tf.keras.optimizers.Nadam(), loss=\"binary_crossentropy\",  metrics=[tf.keras.metrics.Precision(top_k=1, name=\"First_precision\")])    \n",
    "model.fit(train_dataset, validation_data = test_dataset, epochs=100, steps_per_epoch=25000, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet tests whether the first guess found by model is correct in a subset of riddles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "def first_guess_right(maxplays = 1000, model = model, guesses = guesses, riddles = riddles, text_vec_layer = text_vec_layer):\n",
    "    total_correct = 0\n",
    "    for i in range(0, maxplays):\n",
    "        word = riddles[-i]\n",
    "        guess = guesses[-i]\n",
    "        right_choices = list(guess)\n",
    "        x = model.predict(text_vec_layer([word]))\n",
    "        letter = alphabet[tf.argmax(x[0])]\n",
    "        if letter in right_choices:\n",
    "            total_correct += 1\n",
    "    return total_correct/maxplays\n",
    "first_guess_right()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
